{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cd10929",
   "metadata": {},
   "source": [
    "# <div style=\"text-align: right\"> FIT1043 Introduction to Data Science<br>Assignment 2 </div>\n",
    "<div style=\"text-align: right\"> Brandon Yong Hoong Tak<br>32025963<br>20<sup>th</sup> April 2022 </div>\n",
    "<div>_______________________________________________________________________________________________________________________________</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45ba145",
   "metadata": {},
   "source": [
    "# Section 1 Introduction\n",
    "The goal of this assignment is to conduct predective analysis on a set of essay feature data using rudementary machine learning techniques. In doing so, I hope to gain a better understanding of how predictive models such as support vector machines(SVM) operate and how best to evaluate their performance. Ultimately, for us to achieve our goal, we must perform multiple tasks ranging from importing the necessary tools and libraries to manipulate our data, creating and preprocessing our train/test datasets, training our predictive model and evaluating the quality of its predictions, all of which will be documented in further detail within this report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d441ab",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Importing Libraries\n",
    "Predictive analysis in python is heavily reliant on the use of libraries, as such we will import a few key libraries for the purposes of this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfa30df",
   "metadata": {},
   "source": [
    "### General Imports\n",
    "Here we will import pandas, numpy and matplotlib to manage and visualize our data, as well as warnings to filter out warning prompts that are ocasionally raised when running certain code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f1a756de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a922b2",
   "metadata": {},
   "source": [
    "### Specific Sklearn/Scipy Imports\n",
    "Here we will import the specific functions we will use from specific modules from the sklearn library. Most notably, we will require the SVC function, as it will serve as our main model/estimator, train_test_split in order to partition our data as well as the confusion matrix and cohen_kappa_score to quantify our models performance. We also need to import the stats module from SciPy to preprocess some of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7bb3c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, RepeatedStratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, make_scorer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from collections import Counter\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e97727",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Reading CSV Files\n",
    "Here we will read the CSV file 'FIT1043-Essay-Features.csv' which contains the necesarry data for us to build, train and test our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eae0cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Essays_df = pd.read_csv('FIT1043-Essay-Features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb6f91bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1457</td>\n",
       "      <td>2153</td>\n",
       "      <td>426</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.053991</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>26.625000</td>\n",
       "      <td>423.995272</td>\n",
       "      <td>0.995294</td>\n",
       "      <td>207</td>\n",
       "      <td>0.485915</td>\n",
       "      <td>105</td>\n",
       "      <td>0.246479</td>\n",
       "      <td>424</td>\n",
       "      <td>412</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>503</td>\n",
       "      <td>1480</td>\n",
       "      <td>292</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5.068493</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>26.545455</td>\n",
       "      <td>290.993103</td>\n",
       "      <td>0.996552</td>\n",
       "      <td>148</td>\n",
       "      <td>0.506849</td>\n",
       "      <td>77</td>\n",
       "      <td>0.263699</td>\n",
       "      <td>356</td>\n",
       "      <td>345</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>253</td>\n",
       "      <td>3964</td>\n",
       "      <td>849</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>4.669022</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>17.326531</td>\n",
       "      <td>843.990544</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>285</td>\n",
       "      <td>0.335689</td>\n",
       "      <td>130</td>\n",
       "      <td>0.153121</td>\n",
       "      <td>750</td>\n",
       "      <td>750</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>107</td>\n",
       "      <td>988</td>\n",
       "      <td>210</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.704762</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>207.653784</td>\n",
       "      <td>0.988828</td>\n",
       "      <td>112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>62</td>\n",
       "      <td>0.295238</td>\n",
       "      <td>217</td>\n",
       "      <td>209</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1450</td>\n",
       "      <td>3139</td>\n",
       "      <td>600</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>5.231667</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>594.652150</td>\n",
       "      <td>0.991087</td>\n",
       "      <td>255</td>\n",
       "      <td>0.425000</td>\n",
       "      <td>165</td>\n",
       "      <td>0.275000</td>\n",
       "      <td>702</td>\n",
       "      <td>677</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>1151</td>\n",
       "      <td>2404</td>\n",
       "      <td>467</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5.147752</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>21.227273</td>\n",
       "      <td>462.987069</td>\n",
       "      <td>0.991407</td>\n",
       "      <td>200</td>\n",
       "      <td>0.428266</td>\n",
       "      <td>113</td>\n",
       "      <td>0.241970</td>\n",
       "      <td>529</td>\n",
       "      <td>519</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>1015</td>\n",
       "      <td>1182</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>4.904564</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>15.062500</td>\n",
       "      <td>238.655462</td>\n",
       "      <td>0.990272</td>\n",
       "      <td>94</td>\n",
       "      <td>0.390041</td>\n",
       "      <td>67</td>\n",
       "      <td>0.278008</td>\n",
       "      <td>293</td>\n",
       "      <td>283</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>1345</td>\n",
       "      <td>1814</td>\n",
       "      <td>363</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4.997245</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>27.923077</td>\n",
       "      <td>362.329640</td>\n",
       "      <td>0.998153</td>\n",
       "      <td>170</td>\n",
       "      <td>0.468320</td>\n",
       "      <td>107</td>\n",
       "      <td>0.294766</td>\n",
       "      <td>427</td>\n",
       "      <td>415</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>344</td>\n",
       "      <td>1427</td>\n",
       "      <td>287</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.972125</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>22.076923</td>\n",
       "      <td>284.657277</td>\n",
       "      <td>0.991837</td>\n",
       "      <td>144</td>\n",
       "      <td>0.501742</td>\n",
       "      <td>83</td>\n",
       "      <td>0.289199</td>\n",
       "      <td>323</td>\n",
       "      <td>312</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>1077</td>\n",
       "      <td>2806</td>\n",
       "      <td>542</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5.177122</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>24.636364</td>\n",
       "      <td>538.988889</td>\n",
       "      <td>0.994444</td>\n",
       "      <td>284</td>\n",
       "      <td>0.523985</td>\n",
       "      <td>155</td>\n",
       "      <td>0.285978</td>\n",
       "      <td>596</td>\n",
       "      <td>575</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1332 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      essayid  chars  words  commas  apostrophes  punctuations  \\\n",
       "0        1457   2153    426      14            6             0   \n",
       "1         503   1480    292       9            7             0   \n",
       "2         253   3964    849      19           26             1   \n",
       "3         107    988    210       8            7             0   \n",
       "4        1450   3139    600      13            8             0   \n",
       "...       ...    ...    ...     ...          ...           ...   \n",
       "1327     1151   2404    467      16           10             0   \n",
       "1328     1015   1182    241       0           14             0   \n",
       "1329     1345   1814    363       5           11             0   \n",
       "1330      344   1427    287       5            8             0   \n",
       "1331     1077   2806    542      24            6             0   \n",
       "\n",
       "      avg_word_length  sentences  questions  avg_word_sentence         POS  \\\n",
       "0            5.053991         16          0          26.625000  423.995272   \n",
       "1            5.068493         11          0          26.545455  290.993103   \n",
       "2            4.669022         49          2          17.326531  843.990544   \n",
       "3            4.704762         12          0          17.500000  207.653784   \n",
       "4            5.231667         24          1          25.000000  594.652150   \n",
       "...               ...        ...        ...                ...         ...   \n",
       "1327         5.147752         22          0          21.227273  462.987069   \n",
       "1328         4.904564         16          0          15.062500  238.655462   \n",
       "1329         4.997245         13          3          27.923077  362.329640   \n",
       "1330         4.972125         13          1          22.076923  284.657277   \n",
       "1331         5.177122         22          3          24.636364  538.988889   \n",
       "\n",
       "      POS/total_words  prompt_words  prompt_words/total_words  synonym_words  \\\n",
       "0            0.995294           207                  0.485915            105   \n",
       "1            0.996552           148                  0.506849             77   \n",
       "2            0.994100           285                  0.335689            130   \n",
       "3            0.988828           112                  0.533333             62   \n",
       "4            0.991087           255                  0.425000            165   \n",
       "...               ...           ...                       ...            ...   \n",
       "1327         0.991407           200                  0.428266            113   \n",
       "1328         0.990272            94                  0.390041             67   \n",
       "1329         0.998153           170                  0.468320            107   \n",
       "1330         0.991837           144                  0.501742             83   \n",
       "1331         0.994444           284                  0.523985            155   \n",
       "\n",
       "      synonym_words/total_words  unstemmed  stemmed  score  \n",
       "0                      0.246479        424      412      4  \n",
       "1                      0.263699        356      345      4  \n",
       "2                      0.153121        750      750      4  \n",
       "3                      0.295238        217      209      3  \n",
       "4                      0.275000        702      677      4  \n",
       "...                         ...        ...      ...    ...  \n",
       "1327                   0.241970        529      519      4  \n",
       "1328                   0.278008        293      283      3  \n",
       "1329                   0.294766        427      415      3  \n",
       "1330                   0.289199        323      312      3  \n",
       "1331                   0.285978        596      575      4  \n",
       "\n",
       "[1332 rows x 19 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Essays_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2703345",
   "metadata": {},
   "source": [
    "### Basic Description of Data\n",
    "Here we can see some rudementary statistics on each column of our essay dataframe. This dataframe contains 19 columns and 1332 rows of entries/essays. Aside from essayid and score, the column values within in this dataset vary wildly in numerical range and can be normally distributed across a continious range, these values will server as our features/inputs when building our model. esssayid will be excluded as it has no correlation to how an essay is graded while score will become our label/output as it is the target variable we intend to predict. Additionally, this dataset also posseses a great number of outliers, this can be seen from the maximum values in a few columns like chars and sentences whose maximum values far exceed their median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c2e14a76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1332, 19)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Essays_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a39d214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.00000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>905.27027</td>\n",
       "      <td>2101.745495</td>\n",
       "      <td>424.485736</td>\n",
       "      <td>14.667417</td>\n",
       "      <td>8.141141</td>\n",
       "      <td>0.47973</td>\n",
       "      <td>4.939762</td>\n",
       "      <td>19.704204</td>\n",
       "      <td>1.222973</td>\n",
       "      <td>23.884687</td>\n",
       "      <td>420.596542</td>\n",
       "      <td>0.989935</td>\n",
       "      <td>198.913664</td>\n",
       "      <td>0.469164</td>\n",
       "      <td>110.16967</td>\n",
       "      <td>0.263846</td>\n",
       "      <td>468.987988</td>\n",
       "      <td>455.507508</td>\n",
       "      <td>3.427177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>526.68760</td>\n",
       "      <td>865.963750</td>\n",
       "      <td>171.873730</td>\n",
       "      <td>10.920781</td>\n",
       "      <td>6.124520</td>\n",
       "      <td>1.27168</td>\n",
       "      <td>0.231071</td>\n",
       "      <td>19.202731</td>\n",
       "      <td>1.847446</td>\n",
       "      <td>11.160020</td>\n",
       "      <td>170.985111</td>\n",
       "      <td>0.007308</td>\n",
       "      <td>82.729266</td>\n",
       "      <td>0.052466</td>\n",
       "      <td>43.96192</td>\n",
       "      <td>0.038870</td>\n",
       "      <td>159.447449</td>\n",
       "      <td>155.751220</td>\n",
       "      <td>0.774275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.231322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084112</td>\n",
       "      <td>35.647059</td>\n",
       "      <td>0.924771</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.288889</td>\n",
       "      <td>11.00000</td>\n",
       "      <td>0.027299</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>442.75000</td>\n",
       "      <td>1527.250000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.791679</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>305.406284</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.435709</td>\n",
       "      <td>81.00000</td>\n",
       "      <td>0.238423</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>350.750000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>914.50000</td>\n",
       "      <td>2029.500000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>4.946059</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.030331</td>\n",
       "      <td>406.982869</td>\n",
       "      <td>0.991572</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.465852</td>\n",
       "      <td>107.50000</td>\n",
       "      <td>0.262872</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>448.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1369.75000</td>\n",
       "      <td>2613.500000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.092938</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.048234</td>\n",
       "      <td>520.739458</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>134.00000</td>\n",
       "      <td>0.288277</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>561.250000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1799.00000</td>\n",
       "      <td>6142.000000</td>\n",
       "      <td>1170.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>26.00000</td>\n",
       "      <td>5.681429</td>\n",
       "      <td>642.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>303.000000</td>\n",
       "      <td>1158.984563</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>669.000000</td>\n",
       "      <td>0.961207</td>\n",
       "      <td>355.00000</td>\n",
       "      <td>0.465517</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          essayid        chars        words       commas  apostrophes  \\\n",
       "count  1332.00000  1332.000000  1332.000000  1332.000000  1332.000000   \n",
       "mean    905.27027  2101.745495   424.485736    14.667417     8.141141   \n",
       "std     526.68760   865.963750   171.873730    10.920781     6.124520   \n",
       "min       0.00000   169.000000    36.000000     0.000000     2.000000   \n",
       "25%     442.75000  1527.250000   310.000000     7.000000     4.000000   \n",
       "50%     914.50000  2029.500000   411.000000    13.000000     6.000000   \n",
       "75%    1369.75000  2613.500000   525.000000    21.000000    11.000000   \n",
       "max    1799.00000  6142.000000  1170.000000    72.000000    51.000000   \n",
       "\n",
       "       punctuations  avg_word_length    sentences    questions  \\\n",
       "count    1332.00000      1332.000000  1332.000000  1332.000000   \n",
       "mean        0.47973         4.939762    19.704204     1.222973   \n",
       "std         1.27168         0.231071    19.202731     1.847446   \n",
       "min         0.00000         2.231322     0.000000     0.000000   \n",
       "25%         0.00000         4.791679    13.000000     0.000000   \n",
       "50%         0.00000         4.946059    18.000000     1.000000   \n",
       "75%         0.00000         5.092938    24.000000     2.000000   \n",
       "max        26.00000         5.681429   642.000000    17.000000   \n",
       "\n",
       "       avg_word_sentence          POS  POS/total_words  prompt_words  \\\n",
       "count        1332.000000  1332.000000      1332.000000   1332.000000   \n",
       "mean           23.884687   420.596542         0.989935    198.913664   \n",
       "std            11.160020   170.985111         0.007308     82.729266   \n",
       "min             1.084112    35.647059         0.924771     14.000000   \n",
       "25%            19.142857   305.406284         0.987758    144.000000   \n",
       "50%            22.030331   406.982869         0.991572    193.000000   \n",
       "75%            26.048234   520.739458         0.994425    246.000000   \n",
       "max           303.000000  1158.984563         1.000000    669.000000   \n",
       "\n",
       "       prompt_words/total_words  synonym_words  synonym_words/total_words  \\\n",
       "count               1332.000000     1332.00000                1332.000000   \n",
       "mean                   0.469164      110.16967                   0.263846   \n",
       "std                    0.052466       43.96192                   0.038870   \n",
       "min                    0.288889       11.00000                   0.027299   \n",
       "25%                    0.435709       81.00000                   0.238423   \n",
       "50%                    0.465852      107.50000                   0.262872   \n",
       "75%                    0.500000      134.00000                   0.288277   \n",
       "max                    0.961207      355.00000                   0.465517   \n",
       "\n",
       "         unstemmed      stemmed        score  \n",
       "count  1332.000000  1332.000000  1332.000000  \n",
       "mean    468.987988   455.507508     3.427177  \n",
       "std     159.447449   155.751220     0.774275  \n",
       "min      48.000000    50.000000     1.000000  \n",
       "25%     361.000000   350.750000     3.000000  \n",
       "50%     463.000000   448.000000     3.000000  \n",
       "75%     581.000000   561.250000     4.000000  \n",
       "max     750.000000   750.000000     6.000000  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Essays_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8c6e4ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(Essays_df['score'], Essays_df['punctuations'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "53eb0f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAOTElEQVR4nO3dYWjc933H8c9XF88i0thSrBUnsyYTh3LhrLnbkQ07jJiGkPaBvUIK84OSB0dcQ3O0xA8cfAkNS2z6YHUfiFHhYpM+6G6UtJ3FCFtic6PIHmVyCbaCKLVr2U5tbJksa6wgpkjfPfBJSM5Jd7o73d13//cLjPT/6eT75kHe/Pjd/87m7gIAxNPV7gEAAPUh4AAQFAEHgKAIOAAERcABIKgHWvlkmzZt8oGBgVY+JQCEd/78+Tvu3nf/eksDPjAwoLGxsVY+JQCEZ2ZXK61zhAIAQRFwAAiKgANAUAQcAIIi4AAQFAFHohWLRWUyGaVSKWUyGRWLxXaPBNSspbcRAp2kWCyqUCjoxIkTevLJJzU6OqpcLidJ2rdvX5unA6qzVn6cbDabde4DR6fIZDIaGhrS7t27F9dKpZLy+bzGx8fbOBmwnJmdd/fsZ9YJOJIqlUppZmZGGzZsWFybnZ1Vd3e35ubm2jgZsNxKAecMHImVTqc1Ojq6bG10dFTpdLpNEwFrQ8CRWIVCQblcTqVSSbOzsyqVSsrlcioUCu0eDagJL2IisRZeqMzn85qYmFA6ndaRI0d4ARNhcAYOAB2u7jNwM9tiZiUzmzCz983sW+X118zsd2b2XvnPV9ZjcABAZbUcoXwq6aC7/8rM/lDSeTN7t/yz77v7P6zfeACAlVQNuLvflHSz/P3HZjYh6ZH1HgwAsLo13YViZgOSvijpl+WlF83sgpmdNLOHVvid/WY2ZmZjU1NTjU0LAFhUc8DNrFfSTyV9291/L+kHkh6VtEP3dujfq/R77n7c3bPunu3r+8y/CAQAqFNNATezDboX7x+7+88kyd1vufucu89L+qGkJ9ZvTADA/Wq5C8UknZA04e7HlqxvXvKwr0riwyMAoIVquQtll6SvS7poZu+V1w5L2mdmOyS5pElJ31iH+QAAK6jlLpRRSVbhR283fxwAQK34LBQACIqAA0BQBBwAgiLgABAUAQeAoAg4AARFwAEgKAIOAEERcAAIioADQFAEHACCIuAAEBQBB4CgCDgABEXAASAoAg4AQRFwAAiKgANAUAQcAIIi4AAQFAEHgKAIOAAERcABICgCDgBBEXAACIqAA0BQBBwAgiLgABAUAQeAoKoG3My2mFnJzCbM7H0z+1Z5/XNm9q6Z/ab89aH1HxcAsKCWHfinkg66e1rSX0v6ppk9LullSWfc/TFJZ8rXAIAWqRpwd7/p7r8qf/+xpAlJj0jaK+lH5Yf9SNLfrtOMAIAK1nQGbmYDkr4o6ZeSPu/uN6V7kZf0Jyv8zn4zGzOzsampqQbHBQAsqDngZtYr6aeSvu3uv6/199z9uLtn3T3b19dXz4wAgApqCriZbdC9eP/Y3X9WXr5lZpvLP98s6fb6jAgAqKSWu1BM0glJE+5+bMmPRiQ9X/7+eUmnmj8eAGAlD9TwmF2Svi7popm9V147LOm7kn5iZjlJ1yR9bV0mBABUVDXg7j4qyVb48ZeaOw4AoFa8ExMAgiLgABAUAQeAoAg4AARFwAEgKAIOAEERcAAIioADQFAEHACCIuAAEBQBB4CgCDgABEXAASAoAg4AQRFwAAiKgANAUAQcAIIi4AAQFAEHgKAIOAAERcABICgCDgBBEXAACIqAA0BQBBwAgiLgABAUAQeAoAg4AARFwAEgqKoBN7OTZnbbzMaXrL1mZr8zs/fKf76yvmMCAO5Xyw78TUnPVlj/vrvvKP95u7ljAQCqqRpwd/+FpA9bMAsAYA0aOQN/0cwulI9YHlrpQWa238zGzGxsamqqgacDACxVb8B/IOlRSTsk3ZT0vZUe6O7H3T3r7tm+vr46nw4AcL+6Au7ut9x9zt3nJf1Q0hPNHQsAUE1dATezzUsuvyppfKXHAgDWxwPVHmBmRUlPSdpkZh9I+o6kp8xshySXNCnpG+s3IgCgkqoBd/d9FZZPrMMsAIA14J2YABAUAQeAoAg4AARFwAEgKAIOAEERcAAIioADQFAEHACCIuAAEBQBB4CgCDgABEXAASAoAg4AQRFwAAiKgANAUAQcAIIi4AAQFAEHgKAIOAAERcABICgCDgBBEXAACIqAA0BQBBwAgiLgABAUAQeAoAg4AARFwAEgKAIOAEFVDbiZnTSz22Y2vmTtc2b2rpn9pvz1ofUdE1gfxWJRmUxGqVRKmUxGxWKx3SMBNatlB/6mpGfvW3tZ0hl3f0zSmfI1EEqxWFShUNDQ0JBmZmY0NDSkQqFAxBGGuXv1B5kNSPpXd8+Ur38t6Sl3v2lmmyX9h7t/odrfk81mfWxsrMGRgebIZDIaGhrS7t27F9dKpZLy+bzGx8dX+U2gtczsvLtnP7NeZ8A/cvc/XvLz/3b3iscoZrZf0n5J6u/v/8urV6/W9R8ANFsqldLMzIw2bNiwuDY7O6vu7m7Nzc21cTJguZUCvu4vYrr7cXfPunu2r69vvZ8OqFk6ndbo6OiytdHRUaXT6TZNBKxNvQG/VT46Ufnr7eaNBLRGoVBQLpdTqVTS7OysSqWScrmcCoVCu0cDavJAnb83Iul5Sd8tfz3VtImAFtm3b58kKZ/Pa2JiQul0WkeOHFlcBzpd1TNwMytKekrSJkm3JH1H0r9I+omkfknXJH3N3T+s9mS8iAkAa7fSGXjVHbi7r7Qd+VLDUwEA6sY7MQEgKAIOAEERcAAIioADQFAEHACCIuAAEBQBR6Ll83l1d3fLzNTd3a18Pt/ukYCaEXAkVj6f1/DwsI4eParp6WkdPXpUw8PDRBxh1PRphM3COzHRSbq7u3X06FG99NJLi2vHjh3T4cOHNTMz08bJgOUa+jjZZiHg6CRmpunpaT344IOLa5988ol6enrUyv8vgGra9nGyQKfauHGjhoeHl60NDw9r48aNbZoIWJt6P40QCO+FF17QoUOHJEkHDhzQ8PCwDh06pAMHDrR5MqA2BByJNTQ0JEk6fPiwDh48qI0bN+rAgQOL60Cn4wwcADocZ+BABYODgzKzxT+Dg4PtHgmoGQFHYg0ODurixYvas2ePpqamtGfPHl28eJGIIwwCjsRaiPepU6e0adMmnTp1ajHiQAQEHIl24sSJVa+BTkbAkWi5XG7Va6CTEXAk1vbt2zUyMqK9e/fqzp072rt3r0ZGRrR9+/Z2jwbUhPvAkVgXLlzQ4OCgRkZG1NfXJ+le1C9cuNDmyYDasANHon300UerXgOdjIAjsfr7+3X9+nXt3LlTN27c0M6dO3X9+nX19/e3ezSgJgQcibUQ77Nnz2rz5s06e/bsYsSBCAg4Eu2tt95a9RroZAQcifbcc8+teg10MgKOxNqyZYvOnTunXbt26ebNm9q1a5fOnTunLVu2tHs0oCbcRojEunbtmvr7+3Xu3Dk9/PDDku5F/dq1a22eDKgNAUeiEWtE1lDAzWxS0seS5iR9WunzaoFO1tvbq+np6cXrnp4e3b17t40TAbVrxhn4bnffQbwRzUK8BwYGdOnSJQ0MDGh6elq9vb3tHg2oCUcoSKyFeF+5ckWSdOXKFW3dulWTk5PtHQyoUaM7cJf0jpmdN7P9lR5gZvvNbMzMxqamphp8OqC5Tp8+veo10MkaDfgud/8LSV+W9E0z+5v7H+Dux9096+7ZhQ8MAjrF008/veo10MkaCri73yh/vS3p55KeaMZQQCv09PRocnJSW7du1eXLlxePT3p6eto9GlCTus/AzaxHUpe7f1z+/hlJf9+0yYB1dvfuXfX29mpyclLbtm2TxF0oiKWRFzE/L+nnZrbw9/yTu/9bU6YCWoRYI7K6A+7uv5X0502cBWi58gZkGXdvwyTA2vFZKEishXh3dXXp9OnT6urqWrYOdDruA0eidXV1aW5uTpI0NzenVCql+fn5Nk8F1IYdOBLtnXfeWfUa6GQEHIn2zDPPrHoNdDICjkSbn59XKpXSmTNnOD5BOJyBI7HcXWam+fn5Ze/A5C4URMEOHACCIuBIrKW3C77++usV14FORsCReO6uV155haMThEPAkWhLd96VroFOZq3cdWSzWR8bG2vZ8wGrWTgqWfr/QKU1oN3M7Hylf/WMHTgSz8z0xhtvcPaNcAg4EmvpLvvVV1+tuA50Mu4DR6IRa0RGwJFofJwsIuMIBYm1NN6PPvpoxXWgk7EDR+JVugsFiIAdOBJt6c670jXQyQg4Eu3y5curXgOdjIAj8cxM27Zt4/gE4RBwJNbSs++lO2/uQkEUBBwAgiLgSKyVjkw4SkEU3EaIxOM2QkTFDhwAgiLgABAURyhIPI5NEBU7cCTWSrcLchshomAHjkQj1oisoR24mT1rZr82s0tm9nKzhgIAVFd3wM0sJekfJX1Z0uOS9pnZ480aDACwukZ24E9IuuTuv3X3/5X0z5L2NmcsAEA1jZyBPyLp+pLrDyT91f0PMrP9kvZLUn9/fwNPh8R67Y/aPUHzvPY/7Z4A/480EvBK91595hUhdz8u6bgkZbNZXjHC2hE9oKJGjlA+kLRlyfWfSrrR2DgAgFo1EvD/kvSYmW01sz+Q9HeSRpozFgCgmrqPUNz9UzN7UdK/S0pJOunu7zdtMgDAqhp6I4+7vy3p7SbNAgBYA95KDwBBEXAACIqAA0BQBBwAgrJWfhqbmU1JutqyJwRqt0nSnXYPAazgz9y97/7FlgYc6FRmNubu2XbPAawFRygAEBQBB4CgCDhwz/F2DwCsFWfgABAUO3AACIqAA0BQBByJZmYnzey2mY23exZgrQg4ku5NSc+2ewigHgQciebuv5D0YbvnAOpBwAEgKAIOAEERcAAIioADQFAEHIlmZkVJ/ynpC2b2gZnl2j0TUCveSg8AQbEDB4CgCDgABEXAASAoAg4AQRFwAAiKgANAUAQcAIL6P82o4Xa2sukTAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(Essays_df['punctuations'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5ec123",
   "metadata": {},
   "source": [
    "This outlier issue becomes even more aparent when plotting the bar chart and box plot of certain columns. In this intance we can clearly see that there is a very strong outlier (26) that exists far beyond the maximum of our interquatile range. This is also causing our bar chart to skew slightly to the right. These outliers may damage the overall performance of our model as they do not represent the general trend of the data and effect our scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae92a61",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Winsorize Outliers\n",
    "As mentioned previously our data contains a large number of outliers that will effect our scaling later when we attempt to scale/normalize our data to a set range. To rectify this, we can winsorize the data in each of our feature columns, reducing the effect of extreme values. (e.g. impute outlier values above a certain threshold to a different value within that threshold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b0c17734",
   "metadata": {},
   "outputs": [],
   "source": [
    "Win_df = Essays_df.iloc[:, 1:18] #not inclusive of essayid or score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb0d2e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Win_df.columns: \n",
    "    Win_df[col] = stats.mstats.winsorize(Win_df[col], limits=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b3d7e4",
   "metadata": {},
   "source": [
    "for each column in our data frame, we winsorize each value above the 1st percentile to a value within the 1st percentile and values below the 99th percentile to a value within the 99th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8b4dbb97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chars</th>\n",
       "      <th>words</th>\n",
       "      <th>commas</th>\n",
       "      <th>apostrophes</th>\n",
       "      <th>punctuations</th>\n",
       "      <th>avg_word_length</th>\n",
       "      <th>sentences</th>\n",
       "      <th>questions</th>\n",
       "      <th>avg_word_sentence</th>\n",
       "      <th>POS</th>\n",
       "      <th>POS/total_words</th>\n",
       "      <th>prompt_words</th>\n",
       "      <th>prompt_words/total_words</th>\n",
       "      <th>synonym_words</th>\n",
       "      <th>synonym_words/total_words</th>\n",
       "      <th>unstemmed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "      <td>1332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2096.228228</td>\n",
       "      <td>423.671171</td>\n",
       "      <td>14.602853</td>\n",
       "      <td>8.084084</td>\n",
       "      <td>0.445946</td>\n",
       "      <td>4.941591</td>\n",
       "      <td>19.205706</td>\n",
       "      <td>1.197447</td>\n",
       "      <td>23.592304</td>\n",
       "      <td>419.760480</td>\n",
       "      <td>0.990049</td>\n",
       "      <td>198.289790</td>\n",
       "      <td>0.468816</td>\n",
       "      <td>109.852853</td>\n",
       "      <td>0.263815</td>\n",
       "      <td>469.277027</td>\n",
       "      <td>455.780781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>840.372890</td>\n",
       "      <td>167.839012</td>\n",
       "      <td>10.669297</td>\n",
       "      <td>5.864522</td>\n",
       "      <td>0.964863</td>\n",
       "      <td>0.215218</td>\n",
       "      <td>8.573001</td>\n",
       "      <td>1.715942</td>\n",
       "      <td>7.327806</td>\n",
       "      <td>166.956204</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>79.785061</td>\n",
       "      <td>0.049850</td>\n",
       "      <td>42.506084</td>\n",
       "      <td>0.037448</td>\n",
       "      <td>158.750698</td>\n",
       "      <td>155.099958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>389.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.414634</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>75.657658</td>\n",
       "      <td>0.960951</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.350575</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.175050</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1527.250000</td>\n",
       "      <td>310.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.791679</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.142857</td>\n",
       "      <td>305.406284</td>\n",
       "      <td>0.987758</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>0.435709</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.238423</td>\n",
       "      <td>361.000000</td>\n",
       "      <td>350.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2029.500000</td>\n",
       "      <td>411.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.946059</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>22.030331</td>\n",
       "      <td>406.982869</td>\n",
       "      <td>0.991572</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>0.465852</td>\n",
       "      <td>107.500000</td>\n",
       "      <td>0.262872</td>\n",
       "      <td>463.000000</td>\n",
       "      <td>448.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2613.500000</td>\n",
       "      <td>525.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.092938</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.048234</td>\n",
       "      <td>520.739458</td>\n",
       "      <td>0.994425</td>\n",
       "      <td>246.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.288277</td>\n",
       "      <td>581.000000</td>\n",
       "      <td>561.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4484.000000</td>\n",
       "      <td>921.000000</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.432432</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>59.666667</td>\n",
       "      <td>912.993435</td>\n",
       "      <td>0.998792</td>\n",
       "      <td>436.000000</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>238.000000</td>\n",
       "      <td>0.358382</td>\n",
       "      <td>750.000000</td>\n",
       "      <td>750.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             chars        words       commas  apostrophes  punctuations  \\\n",
       "count  1332.000000  1332.000000  1332.000000  1332.000000   1332.000000   \n",
       "mean   2096.228228   423.671171    14.602853     8.084084      0.445946   \n",
       "std     840.372890   167.839012    10.669297     5.864522      0.964863   \n",
       "min     389.000000    78.000000     0.000000     2.000000      0.000000   \n",
       "25%    1527.250000   310.000000     7.000000     4.000000      0.000000   \n",
       "50%    2029.500000   411.000000    13.000000     6.000000      0.000000   \n",
       "75%    2613.500000   525.000000    21.000000    11.000000      0.000000   \n",
       "max    4484.000000   921.000000    51.000000    29.000000      5.000000   \n",
       "\n",
       "       avg_word_length    sentences    questions  avg_word_sentence  \\\n",
       "count      1332.000000  1332.000000  1332.000000        1332.000000   \n",
       "mean          4.941591    19.205706     1.197447          23.592304   \n",
       "std           0.215218     8.573001     1.715942           7.327806   \n",
       "min           4.414634     3.000000     0.000000          13.000000   \n",
       "25%           4.791679    13.000000     0.000000          19.142857   \n",
       "50%           4.946059    18.000000     1.000000          22.030331   \n",
       "75%           5.092938    24.000000     2.000000          26.048234   \n",
       "max           5.432432    43.000000     8.000000          59.666667   \n",
       "\n",
       "               POS  POS/total_words  prompt_words  prompt_words/total_words  \\\n",
       "count  1332.000000      1332.000000   1332.000000               1332.000000   \n",
       "mean    419.760480         0.990049    198.289790                  0.468816   \n",
       "std     166.956204         0.006597     79.785061                  0.049850   \n",
       "min      75.657658         0.960951     36.000000                  0.350575   \n",
       "25%     305.406284         0.987758    144.000000                  0.435709   \n",
       "50%     406.982869         0.991572    193.000000                  0.465852   \n",
       "75%     520.739458         0.994425    246.000000                  0.500000   \n",
       "max     912.993435         0.998792    436.000000                  0.603960   \n",
       "\n",
       "       synonym_words  synonym_words/total_words    unstemmed      stemmed  \n",
       "count    1332.000000                1332.000000  1332.000000  1332.000000  \n",
       "mean      109.852853                   0.263815   469.277027   455.780781  \n",
       "std        42.506084                   0.037448   158.750698   155.099958  \n",
       "min        21.000000                   0.175050   105.000000   104.000000  \n",
       "25%        81.000000                   0.238423   361.000000   350.750000  \n",
       "50%       107.500000                   0.262872   463.000000   448.000000  \n",
       "75%       134.000000                   0.288277   581.000000   561.250000  \n",
       "max       238.000000                   0.358382   750.000000   750.000000  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Win_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b19e5bd",
   "metadata": {},
   "source": [
    "Here we can see post-winsorization that the max values in each column have been trimmed down to a more reasonable value, reducing the skewness of each features distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "22325ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(Essays_df['score'], Win_df['punctuations'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "98e9e4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(Win_df['punctuations'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbed21f8",
   "metadata": {},
   "source": [
    "While there still exists outliers within our dataset, these are no where near as extreme as the ones we have winsorized. As such, these values can be taken as natural outliers that are reflective of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7773443",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Section 2 Supervised Learning\n",
    "In the field of machine learning, methods used to train a predictive model can fall into one of two categories, supervised and unsupervised learning. In supervised learning, a predictor model is trained on a dataset containing both a set of input variables and a set of known output variables. This is known as the concept of 'labelling' or 'labelled data' whereby all data is labelled and inputs are already mapped to the correct output. This allows a model to learn how to predict values from previously unseen data by emulating the mapping function it learnt from the labelled training data. Supervised learning is primarily divided into two categories that being classification problems for discrete output variables and regression problems for continious outputs.\n",
    "\n",
    "Source: https://www.javatpoint.com/supervised-machine-learning, https://www.ibm.com/cloud/learn/supervised-learning#:~:text=Supervised%20learning%2C%20also%20known%20as,data%20or%20predict%20outcomes%20accurately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f961e4d",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Creating Train and Test Datasets\n",
    "It's important that we create seperate datasets for the purposes of machine learning in order to create an effective model. The goal of machine learning is to train a model to effectively generalize a pattern and make accurate predictions based on unseen data. If we were to train and test our model on the entire data set we would likely introduce overfitting into our model. We would also not be able to evaluate our model accurately as it might have developed hyper-specific patterns that allow it to perform well on the data it has seen, but may perform poorly when encountering new data. As such, it is important that we partition our dataset into a training set, used solely for the purpose of constructing and training our model, and a test/validation set used exclusively to evaluate the models performance using data it has never seen before.\n",
    "\n",
    "Source: https://blog.roboflow.com/train-test-split/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9c3479cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Win_df.iloc[:, 0:18].values \n",
    "y = Essays_df.iloc[:, 18]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b344c94b",
   "metadata": {},
   "source": [
    "Here we split the original data into features and labels. We grab the features from the dataset we winsorized previously and assign it to the array X while the label 'score' is obtained from the original dataset and assigned to the array y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4aed3b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1332, 17)\n",
      "(1332,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b239c437",
   "metadata": {},
   "source": [
    "As we can see both arrays have the same number of rows but differing columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd26289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5837b46",
   "metadata": {},
   "source": [
    "We can then split our X and y arrays into the X_train and X_test datasets and y_train and y_test datasets respectively. Here i have set a test_size of 20%, as such only 20% of the data will be partitioned into our test sets while the remaining 80% will be allocated to our training sets. I have chosen an 80/20 split to provide the model more datapoints to learn from  as our overall dataset is not very large ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fa78373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class=3, n=437 (32.808%)\n",
      "Class=4, n=486 (36.486%)\n",
      "Class=5, n=44 (3.303%)\n",
      "Class=2, n=83 (6.231%)\n",
      "Class=1, n=13 (0.976%)\n",
      "Class=6, n=2 (0.150%)\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y_train)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y) * 100\n",
    "    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba74cb16",
   "metadata": {},
   "source": [
    "It's worth noting that the distribution of classes/labels in our data set is extremely imbalanced. Just based on our training set, we can see that the top two classes, 3 and 4 ,make up almost 70% of the overall data we for our label 'score'.\n",
    "This may lead to issues later when attempting to train a model that can effectively learn how to predict each class due to their low frequency in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ff2b6b",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Section 3 Classification\n",
    "There are two kinds of classification problems, binary and multi-class. Binary classifications are problems whereby the dataset's label can be only assigned to one of two possible classes such as 'yes' or 'no' or 1 and 0. Multi-class classifications are problems where the dataset's label can be assigned to more than two classes, this assignment for example, is a multi-class classification problem as we are attempting to classify an essay into a grade/score from 1 to 6."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7002af01",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Scaling/Normalising Feature Sets\n",
    "We will scale our data to a standardized range by removing the mean and scaling to unit variance using sklearns StandardScaler() function. As we saw previously, our feature data was dispersed over a wide variety of ranges, possesing differing shapes and distributions, standardizing our data into a set range helps to improve our models performance as many models tend to perform poorly if the individual features are not arranged in a uniform gaussian distibution.\n",
    "\n",
    "Source: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a894993",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3458d8",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# SVM and Kernal Properties\n",
    "SVMs or Support Vector Machines, are a type of linear supervised learning model that is commonly used to solve classification and regresssion problems. unlike a linear regression, it is capable of seperating both linear and non-linearly seperable datasets by defining an n-dimensional plane through the use of support vectors, which are essentially the data points closest to the plane. This n-dimensional plane, also known as a hyperplane, acts as the decision boundry, classifying datapoints on either side of it as seperate classes, the confidence to which it can classify a datapoint as a member of a certain class is based on how great the distance between the datapoint and the hyperplane is, this distance is known as the margin. As such it is the job of the SVM, to determine the best possible hyperplane in order to maximize this margin and classify data more accurately. Once the svm has determined the appropiate hyperplane to seperate the data, it can then use complicated mathematical functions and transformations to map this decision boundary back to two dimensions. Additionally, SVM's are more robust to outliers as opposed to linear regressions and are capable of being used to predict both discrete and continious variables.\n",
    "\n",
    "As mentioned prior, SVM's are heavily reliant on these complicated mathematical operations and transformations in order to seperate non linear data which can result in extremely high computational loads depending on the size and complexity of the data set. Hence, we make use of a kernel to cut short the computational load. A kernel is essentially a special function that allows us to compute a linear or non-linear classifier in higher dimensional spaces without needing to excplicitly transform and map our feature space to a higher dimension. This shortcut is known as the \"kernel trick\" and allows SVM's to compute complicated hyperplanes in high-dimensions with relative ease. Different kernels such as 'linear', 'rbf' and 'sigmoid' kernels use different functions and perform differently when used with different types a datasets. For example, a linear kernel might be able to quickly and accurately seperate a set of linearly seperable data but might struggle when attempting to process non-linear data when compared to a polynomial or rbf kernel.\n",
    "\n",
    "Source: https://monkeylearn.com/blog/introduction-to-support-vector-machines-svm/#:~:text=How%20Does%20SVM%20Work%3F,-The%20basics%20of&text=A%20support%20vector%20machine%20takes,to%20the%20other%20as%20red, https://towardsdatascience.com/https-medium-com-pupalerushikesh-svm-f4b42800e989#:~:text=SVM%20or%20Support%20Vector%20Machine,separates%20the%20data%20into%20classes, https://www.quora.com/What-are-kernels-in-machine-learning-and-SVM-and-why-do-we-need-them, https://towardsdatascience.com/comparative-study-on-classic-machine-learning-algorithms-24f9ff6ab222"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d143d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = SVC(class_weight =\"balanced\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73bf30",
   "metadata": {},
   "source": [
    "Here i am just initializing my model to the Support Vector Classifier (SVC) from the sklearn.svm module. I am not setting any hyperparameters outside of class_weight = \"balanced\" as we will determine which hyperparameters are best suited for my data using a gridsearch with K-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be8b3a",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Exhaustive Gridsearch to Determine Best Model Hyperparameters\n",
    "Here we will run an exhaustive gridsearch across multiple SVC hyperparameters to determine what kernel, C and gamma are best suited for modelling this dataset. This gridsearch makes use of the concept of cross validation which essentially is the process of splitting a dataset into k-number folds/pairs of training and testing data and returning performance metrics for the model when it is trained and tested on each seperate fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df01e567",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = [\n",
    "    {\"kernel\": [\"rbf\", \"linear\"], \"gamma\": [0.1, 0.01, 0.001, 0.001], \"C\": [1, 10, 100, 1000]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11cf34a",
   "metadata": {},
   "source": [
    "Here we can set the grid to a nested dictionary of parameters we want to iterate through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8b60390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "0.6632244867182413\n",
      "{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "kappa_scorer = make_scorer(cohen_kappa_score, weights=\"quadratic\")\n",
    "grid_search = GridSearchCV(estimator = Model, param_grid = grid, cv = 5, n_jobs = -1, verbose = 2, scoring = kappa_scorer)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d327e4a6",
   "metadata": {},
   "source": [
    "We make use of the function make_scorer() in order to create our custom scoring metric for the grid search, which in this case will be the quadratic weighted kappa of our model. We then pass the model we previously initialized into the gridsearcher and fit it to our training data. Once it has completey iterated through every possible combination of parameters defined in our grid, we print our highest scoring parameters along with their score. Beyond that the remaining parameters are as follows: cv = number of k-folds (5), n_jobs = number of cpu cores used for this task (-1 meaning all of them), verbose = details the function will print during execution (2 being a brief summary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "90060123",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, class_weight={1: 68, 2: 13, 3: 5, 4: 5, 5: 12, 6: 36}, gamma=0.001)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = {1:68, 2:13, 3:5, 4:5, 5:12, 6:36}\n",
    "OptimizedModel = SVC(kernel = 'rbf', C = 10, gamma = 0.001, class_weight = weights)\n",
    "OptimizedModel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d1ed3",
   "metadata": {},
   "source": [
    "In my case 90% of the time, the best parameters for this particular dataset are an rbf kernel, a C of 10 and a gamma or 0.001. We can then input these hyperparameters into our new OptimizedModel and begin manually tuning our class_weight. This is necesarry as the frequency of classes that appear in this dataset is extremely imbalanced, as such we have to increase the weight of classes that appear less frequently so it will impact the machine learning algorithm more drastically when encountered during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "47aeba1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6924853539548175\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(cross_val_score(OptimizedModel, X_train, y_train, cv=10, scoring = kappa_scorer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca64fd",
   "metadata": {},
   "source": [
    "Once again we make use of cross-validation in order to properly evaluate our model. This time we return the mean of 10 k-folds resulting in greater consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c7f265",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Predictions and Model Evaluation/Metrics\n",
    "Now that we have our trained model we can begin predicting and validating our data from the test set using different kinds of model evaluation techniques and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d22285e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 5 3 3 2 4 2 4 3 4 4 5 3 4 4 4 3 4 3 1 4 2 6 4 4 3 5 3 3 4 4 3 4 4 3 3\n",
      " 2 4 4 3 4 3 3 3 4 4 4 4 3 4 3 4 4 2 3 3 3 2 4 1 3 2 3 3 5 4 3 3 4 2 4 3 3\n",
      " 4 3 3 4 4 3 3 1 1 4 4 3 4 3 4 3 3 3 3 3 4 4 5 4 2 3 2 4 3 4 4 3 4 2 2 4 4\n",
      " 3 3 3 3 3 3 3 4 3 3 4 3 3 4 4 3 3 3 5 4 2 3 4 4 3 3 3 3 3 3 4 3 3 4 4 4 4\n",
      " 4 4 3 4 3 2 3 2 4 4 3 3 2 3 4 4 4 5 3 2 4 4 4 4 3 4 4 4 3 3 4 4 4 3 4 3 4\n",
      " 3 3 3 4 4 3 4 4 3 3 2 3 3 3 2 4 3 3 3 3 4 3 4 4 1 3 4 4 3 3 3 4 4 4 3 1 4\n",
      " 4 3 1 4 3 3 4 4 3 3 4 4 4 2 4 4 4 4 3 3 3 3 2 3 3 3 3 4 1 4 4 4 3 3 3 3 4\n",
      " 4 3 3 4 3 2 2 3]\n"
     ]
    }
   ],
   "source": [
    "y_pred = OptimizedModel.predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d78e1531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  0,  0,  0,  0],\n",
       "       [ 5,  9, 13,  0,  0,  0],\n",
       "       [ 1, 11, 81, 27,  0,  0],\n",
       "       [ 0,  0, 21, 72,  4,  0],\n",
       "       [ 0,  0,  2, 10,  3,  1],\n",
       "       [ 0,  0,  0,  2,  0,  0]], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca0b16f",
   "metadata": {},
   "source": [
    "The confusion matrix is a summary of all prediction reults within a classification problem. It shows a mixture of True Positives(TP), False Positives(FP), True Negatives(TN) and False Negatives(FN) that have been predicted for any given class. In this case, we have a total of 6 classes, hence we display a 6x6 matrix of how our model has decided to classify each and every essay from our test set. This is important as not only does it allow us to evaluate if our model is making mistakes, but also to visualize how those mistakes and misclassifications are being made. Beyond that, we can also calculate a handful of other important metrics from the confusion matrix such as accuracy, precision, misclassification, sensitivity, specificity and error using the TP, TN, FP, and FN values obtained from the matrix.\n",
    "\n",
    "Source: https://machinelearningmastery.com/confusion-matrix-machine-learning/#:~:text=A%20confusion%20matrix%20is%20a%20summary%20of%20prediction%20results%20on,key%20to%20the%20confusion%20matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a8000867",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6837763519706691"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(y_test, y_pred, weights = 'quadratic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da9072",
   "metadata": {},
   "source": [
    "Quadratic Weighted Kappa (QWK) is a type of statistical metric that quantifies the degree of agreement between two raters. Its score can range from -1 to 1 whereby 1 would represent complete and total agreement, -1 would be complete disagreement, and the values in between would denote some varying degree of agreement:\n",
    "\n",
    "  0  - 0.20   ->  Complete randomness of agreement/no agreement<br>\n",
    "0.21 - 0.39   ->  Minimal agreement<br>\n",
    "0.40 - 0.59   ->  Weak agreement<br>\n",
    "0.60 - 0.79   ->  Moderate agreement<br>\n",
    "0.80 - 0.90   ->  Strong agreement<br>\n",
    "0.91 - 0.99   ->  Near perfect agreement<br>\n",
    "\n",
    "QWK is calculated between a set of of predicted and known values and represents a useful metric for classification when simple metrics such as accuracy are not fully representative of a models performance. In order for a weighted kappa score to be reliable, it is crucial that both raters observed the exact same data and are independant from one another. A good example of this would like judging food at a competition, in order to give a fair evaluation between judges, each judge must taste the same dish(data) and judge the dish independantly from each and other (unbiased).\n",
    "\n",
    "Source: https://www.kaggle.com/code/aroraaman/quadratic-kappa-metric-explained-in-5-simple-steps/notebook, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a3c065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.40      0.31         5\n",
      "           2       0.39      0.33      0.36        27\n",
      "           3       0.69      0.68      0.68       120\n",
      "           4       0.65      0.74      0.69        97\n",
      "           5       0.43      0.19      0.26        16\n",
      "           6       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.63       267\n",
      "   macro avg       0.40      0.39      0.38       267\n",
      "weighted avg       0.62      0.63      0.62       267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99858c",
   "metadata": {},
   "source": [
    "The classification report is also another handy tool for determining your models performance. For example, our dataset has an imbalance of classes, thus it would be more appropriate to observe our percision and recall metrics rather than focus on increasing the accuracy of the model. In this case  we would like to maximize our precision in classifying score 3 and 4 essays as they are the most abudant class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de9429e",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Section 4 Kaggle Submission Task\n",
    "Now that we've trained and evaluated or model we can use it to predict and submit our scores to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a77072f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Kaggle_df = pd.read_csv('FIT1043-Essay-Features-Submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "65f831be",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Kaggle_df.iloc[:, 1:18].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d6bae3",
   "metadata": {},
   "source": [
    "Here we can read the Essay Features Submission csv file into a dataframe and cast the columns 1 to 17 to an array called X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e845ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701903a",
   "metadata": {},
   "source": [
    "Then we perform the same process of standardizing and scaling the data using StandardScaler so our model can predict the scores more accurately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "54ac24d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = OptimizedModel.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "6b741fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df = Kaggle_df.iloc[:, [0]]\n",
    "Submission_df['score'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812f739f",
   "metadata": {},
   "source": [
    "Now we can create the dataframe we wish to export and submit to kaggle by grabbing the first column of our Kaggle dataframe containing the essayid column and adding our y predictions as a seperate column called score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "36f84a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essayid</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1623</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>660</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1596</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>846</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>1226</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>862</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>1562</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>1336</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>1171</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essayid  score\n",
       "0       1623      4\n",
       "1       1143      3\n",
       "2        660      4\n",
       "3       1596      4\n",
       "4        846      4\n",
       "..       ...    ...\n",
       "194     1226      3\n",
       "195      862      4\n",
       "196     1562      4\n",
       "197     1336      3\n",
       "198     1171      3\n",
       "\n",
       "[199 rows x 2 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f155839",
   "metadata": {},
   "outputs": [],
   "source": [
    "Submission_df.to_csv (r'C:\\Users\\brand\\Desktop\\export_dataframe.csv', index = False, header=True) #exporting df to .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "84a2051c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 3 4 4 4 4 3 3 3 3 3 4 4 3 4 4 4 4 4 3 3 3 4 4 4 4 4 4 4 3 2 4 3 3 4 3 5\n",
      " 4 3 3 3 4 3 3 1 3 3 4 4 3 2 4 4 4 3 4 4 4 4 4 2 3 3 4 3 3 4 4 3 4 3 3 4 3\n",
      " 3 3 3 4 4 4 4 3 4 3 4 2 4 4 1 3 3 3 4 3 4 4 4 4 3 3 4 5 2 3 3 4 3 3 5 4 4\n",
      " 3 4 4 4 3 2 4 2 3 4 4 4 3 2 4 3 5 4 3 4 4 1 3 4 4 3 4 1 4 4 4 4 3 3 4 4 3\n",
      " 4 4 4 3 4 3 4 3 3 3 3 4 4 3 2 2 4 5 3 3 2 4 3 4 4 3 4 4 4 3 3 5 4 2 4 4 4\n",
      " 4 3 3 3 3 3 4 3 5 3 4 4 3 3]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b2449",
   "metadata": {},
   "source": [
    "Here's a summary of our predictions for this iteration!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a958e52f",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>\n",
    "\n",
    "# Section 5 Conclusion\n",
    "In conclusion, we have successfully trained a basic SVM predictor model and gone through the necesarry steps in order to verify and evaluate its performance. In doing so, I have gained a deeper understanding of the data modelling and predictive analysis aspects data science as well as exposure to the practical methods, techniques and libraries used by data professionals across the world in the field of multi-class classification as well as pre-processing imbalanced and noisy datasets. I have also gained experience in aspect such as evaluation techniques, model performance metrics as well as independant model evaluation through Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da304e56",
   "metadata": {},
   "source": [
    "<div>_______________________________________________________________________________________________________________________________</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
